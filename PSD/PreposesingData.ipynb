{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589ec587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPROCESSING NOTEBOOK\n",
      "==================================================\n",
      "Tahap preprocessing data untuk dataset Iris\n",
      "Berdasarkan hasil analisis dari DataUnderstanding notebook\n",
      "Penanganan outliers dari multi-model PyCaret (ABOD, KNN, COF)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Notebook ini berisi tahap preprocessing data untuk dataset Iris berdasarkan hasil analisis dari DataUnderstanding notebook, termasuk penanganan outliers yang terdeteksi oleh multi-model PyCaret (ABOD, KNN, COF).\n",
    "\n",
    "print(\"DATA PREPROCESSING NOTEBOOK\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Tahap preprocessing data untuk dataset Iris\")\n",
    "print(\"Berdasarkan hasil analisis dari DataUnderstanding notebook\")\n",
    "print(\"Penanganan outliers dari multi-model PyCaret (ABOD, KNN, COF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68047d2e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries dan Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3997e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyCaret berhasil diimport\n",
      "Libraries berhasil diimport untuk preprocessing\n",
      "Libraries yang tersedia:\n",
      "   • Pandas & NumPy: Data manipulation\n",
      "   • Matplotlib & Seaborn: Visualisasi\n",
      "   • Scikit-learn: Preprocessing tools\n",
      "   • PyCaret: Advanced ML preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Import libraries yang diperlukan untuk preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PyCaret untuk preprocessing dan modeling\n",
    "try:\n",
    "    from pycaret.datasets import get_data\n",
    "    from pycaret.classification import *\n",
    "    from pycaret.anomaly import *\n",
    "    print(\"PyCaret berhasil diimport\")\n",
    "except ImportError:\n",
    "    print(\"PyCaret tidak tersedia. Install dengan: pip install pycaret\")\n",
    "\n",
    "# Atur style untuk visualisasi\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries berhasil diimport untuk preprocessing\")\n",
    "print(\"Libraries yang tersedia:\")\n",
    "print(\"   • Pandas & NumPy: Data manipulation\")\n",
    "print(\"   • Matplotlib & Seaborn: Visualisasi\")\n",
    "print(\"   • Scikit-learn: Preprocessing tools\")\n",
    "print(\"   • PyCaret: Advanced ML preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b3e04",
   "metadata": {},
   "source": [
    "## 2. Load Data dan Hasil Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6660f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING DATA DAN HASIL OUTLIER DETECTION ===\n",
      "Dataset Iris berhasil dimuat dari data_iris.csv\n",
      "\n",
      "Info Dataset:\n",
      "   • Ukuran: 150 baris, 6 kolom\n",
      "   • Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "   • Target: species (0=setosa, 1=versicolor, 2=virginica)\n",
      "\n",
      "Sample Data:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   species species_name  \n",
      "0        0       setosa  \n",
      "1        0       setosa  \n",
      "2        0       setosa  \n",
      "3        0       setosa  \n",
      "4        0       setosa  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset Iris dengan hasil outlier detection dari DataUnderstanding\n",
    "print(\"=== LOADING DATA DAN HASIL OUTLIER DETECTION ===\")\n",
    "\n",
    "try:\n",
    "    # Load data dari file CSV atau PyCaret\n",
    "    try:\n",
    "        # Coba load dari file lokal\n",
    "        df = pd.read_csv('data_iris.csv', delimiter=';')\n",
    "        \n",
    "        # Konversi kolom numerik yang menggunakan koma sebagai decimal separator\n",
    "        numeric_columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                # Konversi koma ke titik untuk decimal\n",
    "                df[col] = df[col].astype(str).str.replace(',', '.').astype(float)\n",
    "        \n",
    "        # Buat kolom species numerik dan species name\n",
    "        df['species'] = df['Class'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "        df['species_name'] = df['Class'].map({'Iris-setosa': 'setosa', 'Iris-versicolor': 'versicolor', 'Iris-virginica': 'virginica'})\n",
    "        \n",
    "        # Rename kolom untuk konsistensi dengan format sklearn\n",
    "        df = df.rename(columns={\n",
    "            'sepal length': 'sepal length (cm)',\n",
    "            'sepal width': 'sepal width (cm)', \n",
    "            'petal length': 'petal length (cm)',\n",
    "            'petal width': 'petal width (cm)'\n",
    "        })\n",
    "        \n",
    "        # Drop kolom yang tidak diperlukan\n",
    "        if 'id' in df.columns:\n",
    "            df = df.drop('id', axis=1)\n",
    "        if 'Class' in df.columns:\n",
    "            df = df.drop('Class', axis=1)\n",
    "        \n",
    "        print(\"Dataset Iris berhasil dimuat dari data_iris.csv\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        # Jika file tidak ditemukan, gunakan dataset Iris dari PyCaret\n",
    "        print(\"File lokal tidak ditemukan. Menggunakan dataset Iris dari PyCaret...\")\n",
    "        try:\n",
    "            df = get_data('iris')\n",
    "            df['species'] = df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2})\n",
    "            df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "            print(\"Dataset Iris berhasil dimuat dari PyCaret\")\n",
    "        except:\n",
    "            print(\"Error: Tidak dapat memuat dataset dari PyCaret\")\n",
    "    \n",
    "    # Define feature columns\n",
    "    features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "    \n",
    "    print(f\"\\nInfo Dataset:\")\n",
    "    print(f\"   • Ukuran: {df.shape[0]} baris, {df.shape[1]} kolom\")\n",
    "    print(f\"   • Features: {features}\")\n",
    "    print(f\"   • Target: species (0=setosa, 1=versicolor, 2=virginica)\")\n",
    "    \n",
    "    # Tampilkan sample data\n",
    "    print(f\"\\nSample Data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7690d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMULASI HASIL OUTLIER DETECTION DARI DATAUNDERSTANDING ===\n",
      "\n",
      "Hasil Outlier Detection dari DataUnderstanding:\n",
      "   • ABOD: 8 outliers (5.3%)\n",
      "     Indices: [41, 62, 100, 106, 108, 117, 131, 134]\n",
      "   • KNN: 8 outliers (5.3%)\n",
      "     Indices: [41, 57, 98, 106, 109, 117, 118, 131]\n",
      "   • COF: 8 outliers (5.3%)\n",
      "     Indices: [14, 15, 22, 33, 41, 44, 106, 117]\n",
      "\n",
      "Consensus Analysis:\n",
      "   • Normal (0 models): 133 data (88.7%)\n",
      "   • 1 model(s) agree: 13 data (8.7%)\n",
      "   • 2 model(s) agree: 1 data (0.7%)\n",
      "   • 3 model(s) agree: 3 data (2.0%)\n",
      "\n",
      "Strong Consensus Outliers (≥2 models agree):\n",
      "   • Count: 4 (2.7%)\n",
      "   • Indices: [41, 106, 117, 131]\n",
      "\n",
      "Perfect Consensus Outliers (all models agree):\n",
      "   • Count: 3 (2.0%)\n",
      "   • Indices: [41, 106, 117]\n"
     ]
    }
   ],
   "source": [
    "# Simulasi hasil outlier detection dari DataUnderstanding\n",
    "# Berdasarkan hasil multi-model PyCaret (ABOD, KNN, COF)\n",
    "print(\"=== SIMULASI HASIL OUTLIER DETECTION DARI DATAUNDERSTANDING ===\")\n",
    "\n",
    "if df is not None:\n",
    "    # Hasil outlier detection berdasarkan analisis DataUnderstanding\n",
    "    # ABOD: [41, 62, 100, 106, 108, 117, 131, 134]\n",
    "    # KNN:  [41, 57, 98, 106, 109, 117, 118, 131]  \n",
    "    # COF:  [14, 15, 22, 33, 41, 44, 106, 117]\n",
    "    \n",
    "    outlier_results = {\n",
    "        'abod': [41, 62, 100, 106, 108, 117, 131, 134],\n",
    "        'knn': [41, 57, 98, 106, 109, 117, 118, 131],\n",
    "        'cof': [14, 15, 22, 33, 41, 44, 106, 117]\n",
    "    }\n",
    "    \n",
    "    # Create outlier columns\n",
    "    for model_name, outlier_indices in outlier_results.items():\n",
    "        df[f'{model_name}_outlier'] = 0\n",
    "        df.loc[outlier_indices, f'{model_name}_outlier'] = 1\n",
    "    \n",
    "    # Calculate consensus scores\n",
    "    df['consensus_score'] = df['abod_outlier'] + df['knn_outlier'] + df['cof_outlier']\n",
    "    df['strong_consensus'] = (df['consensus_score'] >= 2).astype(int)\n",
    "    df['perfect_consensus'] = (df['consensus_score'] == 3).astype(int)\n",
    "    \n",
    "    # Analisis hasil outlier detection\n",
    "    print(f\"\\nHasil Outlier Detection dari DataUnderstanding:\")\n",
    "    for model_name, outlier_indices in outlier_results.items():\n",
    "        outlier_count = len(outlier_indices)\n",
    "        outlier_pct = (outlier_count / len(df)) * 100\n",
    "        print(f\"   • {model_name.upper()}: {outlier_count} outliers ({outlier_pct:.1f}%)\")\n",
    "        print(f\"     Indices: {outlier_indices}\")\n",
    "    \n",
    "    # Consensus analysis\n",
    "    consensus_stats = df['consensus_score'].value_counts().sort_index()\n",
    "    print(f\"\\nConsensus Analysis:\")\n",
    "    for score, count in consensus_stats.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        if score == 0:\n",
    "            print(f\"   • Normal (0 models): {count} data ({pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   • {int(score)} model(s) agree: {count} data ({pct:.1f}%)\")\n",
    "    \n",
    "    # Strong consensus outliers\n",
    "    strong_consensus_count = df['strong_consensus'].sum()\n",
    "    strong_consensus_indices = df[df['strong_consensus'] == 1].index.tolist()\n",
    "    print(f\"\\nStrong Consensus Outliers (≥2 models agree):\")\n",
    "    print(f\"   • Count: {strong_consensus_count} ({strong_consensus_count/len(df)*100:.1f}%)\")\n",
    "    print(f\"   • Indices: {strong_consensus_indices}\")\n",
    "    \n",
    "    # Perfect consensus outliers\n",
    "    perfect_consensus_count = df['perfect_consensus'].sum()\n",
    "    perfect_consensus_indices = df[df['perfect_consensus'] == 1].index.tolist()\n",
    "    print(f\"\\nPerfect Consensus Outliers (all models agree):\")\n",
    "    print(f\"   • Count: {perfect_consensus_count} ({perfect_consensus_count/len(df)*100:.1f}%)\")\n",
    "    print(f\"   • Indices: {perfect_consensus_indices}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Tidak dapat melakukan analisis outlier karena data tidak berhasil dimuat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b0917",
   "metadata": {},
   "source": [
    "## 3. Outlier Treatment Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c9b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STRATEGI PENANGANAN OUTLIERS ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analisis Dampak Treatment Outliers:\n",
      "Strategy             Samples    Outliers Removed   % Removed   \n",
      "-----------------------------------------------------------------\n",
      "Original             150        0                  0.0        %\n",
      "Remove Strong        146        4                  2.7        %\n",
      "Remove Perfect       147        3                  2.0        %\n",
      "Cap IQR              150        0                  0.0        %\n",
      "Winsorization        150        0                  0.0        %\n",
      "\n",
      "Perbandingan Statistik Deskriptif (Sepal Length):\n",
      "Strategy             Mean     Std      Min      Max     \n",
      "-------------------------------------------------------\n",
      "Original             5.84    0.83    4.30    7.90   \n",
      "Remove Strong        5.83    0.80    4.30    7.70   \n",
      "Remove Perfect       5.85    0.81    4.30    7.90   \n",
      "Cap IQR              5.84    0.83    4.30    7.90   \n",
      "Winsorization        5.83    0.78    4.60    7.25   \n",
      "\n",
      "REKOMENDASI TREATMENT STRATEGY:\n",
      "\n",
      "   Berdasarkan analisis outlier detection:\n",
      "   • Strong consensus outliers: 4 (2.7%)\n",
      "   • Perfect consensus outliers: 3 (2.0%)\n",
      "\n",
      "   STRATEGI YANG DIREKOMENDASIKAN: CAP_OUTLIERS\n",
      "   Alasan: Outlier moderate, capping lebih baik dari removal\n",
      "\n",
      "   KHUSUS UNTUK IRIS DATASET:\n",
      "   • Outliers mungkin merupakan variasi natural bunga iris\n",
      "   • Untuk klasifikasi, outliers dapat membantu model robustness\n",
      "   • Direkomendasikan: ORIGINAL atau CAP_OUTLIERS\n",
      "\n",
      "Dataset processed dengan strategi 'cap_outliers' siap digunakan\n"
     ]
    }
   ],
   "source": [
    "# Strategi Penanganan Outliers berdasarkan Consensus Analysis\n",
    "print(\"=== STRATEGI PENANGANAN OUTLIERS ===\")\n",
    "\n",
    "if df is not None and 'consensus_score' in df.columns:\n",
    "    \n",
    "    # Buat beberapa versi dataset dengan treatment berbeda\n",
    "    datasets = {}\n",
    "    \n",
    "    # Dataset 1: Original (tidak ada treatment)\n",
    "    datasets['original'] = df.copy()\n",
    "    \n",
    "    # Dataset 2: Remove Strong Consensus Outliers (≥2 models agree)\n",
    "    datasets['remove_strong'] = df[df['strong_consensus'] == 0].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Dataset 3: Remove Perfect Consensus Outliers (all 3 models agree)\n",
    "    datasets['remove_perfect'] = df[df['perfect_consensus'] == 0].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Dataset 4: Cap outliers menggunakan IQR method pada strong consensus\n",
    "    datasets['cap_outliers'] = df.copy()\n",
    "    \n",
    "    # Untuk dataset cap_outliers, ganti nilai outliers dengan batas IQR\n",
    "    strong_outlier_mask = df['strong_consensus'] == 1\n",
    "    for feature in features:\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Cap outliers\n",
    "        datasets['cap_outliers'].loc[strong_outlier_mask, feature] = np.clip(\n",
    "            datasets['cap_outliers'].loc[strong_outlier_mask, feature],\n",
    "            lower_bound, upper_bound\n",
    "        )\n",
    "    \n",
    "    # Dataset 5: Winsorization (cap pada percentile 5-95)\n",
    "    datasets['winsorize'] = df.copy()\n",
    "    for feature in features:\n",
    "        p5 = df[feature].quantile(0.05)\n",
    "        p95 = df[feature].quantile(0.95)\n",
    "        datasets['winsorize'][feature] = np.clip(datasets['winsorize'][feature], p5, p95)\n",
    "    \n",
    "    # Analisis dampak setiap treatment\n",
    "    print(f\"\\nAnalisis Dampak Treatment Outliers:\")\n",
    "    print(f\"{'Strategy':<20} {'Samples':<10} {'Outliers Removed':<18} {'% Removed':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    original_count = len(datasets['original'])\n",
    "    \n",
    "    for strategy, dataset in datasets.items():\n",
    "        current_count = len(dataset)\n",
    "        removed_count = original_count - current_count\n",
    "        removed_pct = (removed_count / original_count) * 100\n",
    "        \n",
    "        strategy_name = {\n",
    "            'original': 'Original',\n",
    "            'remove_strong': 'Remove Strong',\n",
    "            'remove_perfect': 'Remove Perfect', \n",
    "            'cap_outliers': 'Cap IQR',\n",
    "            'winsorize': 'Winsorization'\n",
    "        }.get(strategy, strategy)\n",
    "        \n",
    "        print(f\"{strategy_name:<20} {current_count:<10} {removed_count:<18} {removed_pct:<11.1f}%\")\n",
    "    \n",
    "    # Tampilkan statistik deskriptif untuk perbandingan\n",
    "    print(f\"\\nPerbandingan Statistik Deskriptif (Sepal Length):\")\n",
    "    print(f\"{'Strategy':<20} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for strategy, dataset in datasets.items():\n",
    "        if len(dataset) > 0:\n",
    "            feature = 'sepal length (cm)'\n",
    "            stats = dataset[feature].describe()\n",
    "            strategy_name = {\n",
    "                'original': 'Original',\n",
    "                'remove_strong': 'Remove Strong',\n",
    "                'remove_perfect': 'Remove Perfect',\n",
    "                'cap_outliers': 'Cap IQR', \n",
    "                'winsorize': 'Winsorization'\n",
    "            }.get(strategy, strategy)\n",
    "            \n",
    "            print(f\"{strategy_name:<20} {stats['mean']:<7.2f} {stats['std']:<7.2f} {stats['min']:<7.2f} {stats['max']:<7.2f}\")\n",
    "    \n",
    "    # Rekomendasi strategy\n",
    "    print(f\"\\nREKOMENDASI TREATMENT STRATEGY:\")\n",
    "    \n",
    "    strong_consensus_count = df['strong_consensus'].sum()\n",
    "    perfect_consensus_count = df['perfect_consensus'].sum()\n",
    "    strong_pct = (strong_consensus_count / len(df)) * 100\n",
    "    perfect_pct = (perfect_consensus_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\n   Berdasarkan analisis outlier detection:\")\n",
    "    print(f\"   • Strong consensus outliers: {strong_consensus_count} ({strong_pct:.1f}%)\")\n",
    "    print(f\"   • Perfect consensus outliers: {perfect_consensus_count} ({perfect_pct:.1f}%)\")\n",
    "    \n",
    "    if perfect_pct < 2:\n",
    "        recommended_strategy = 'original'\n",
    "        reason = \"Outlier percentage sangat rendah, dataset berkualitas baik\"\n",
    "    elif strong_pct < 5:\n",
    "        recommended_strategy = 'cap_outliers'\n",
    "        reason = \"Outlier moderate, capping lebih baik dari removal\"\n",
    "    else:\n",
    "        recommended_strategy = 'remove_strong'\n",
    "        reason = \"Outlier tinggi, perlu removal untuk model stability\"\n",
    "    \n",
    "    print(f\"\\n   STRATEGI YANG DIREKOMENDASIKAN: {recommended_strategy.upper()}\")\n",
    "    print(f\"   Alasan: {reason}\")\n",
    "    \n",
    "    # Untuk Iris dataset yang natural, kita pilih strategi yang mempertahankan data\n",
    "    print(f\"\\n   KHUSUS UNTUK IRIS DATASET:\")\n",
    "    print(f\"   • Outliers mungkin merupakan variasi natural bunga iris\")\n",
    "    print(f\"   • Untuk klasifikasi, outliers dapat membantu model robustness\")\n",
    "    print(f\"   • Direkomendasikan: ORIGINAL atau CAP_OUTLIERS\")\n",
    "    \n",
    "    # Simpan dataset yang direkomendasikan\n",
    "    if recommended_strategy in datasets:\n",
    "        df_processed = datasets[recommended_strategy].copy()\n",
    "        print(f\"\\nDataset processed dengan strategi '{recommended_strategy}' siap digunakan\")\n",
    "    else:\n",
    "        df_processed = datasets['original'].copy()\n",
    "        print(f\"\\nMenggunakan dataset original sebagai fallback\")\n",
    "        \n",
    "else:\n",
    "    print(\"Tidak dapat melakukan treatment outliers karena data atau hasil outlier tidak tersedia\")\n",
    "    df_processed = df.copy() if df is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c0754",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling dan Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397ca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SCALING DAN ENGINEERING ===\n",
      "\n",
      "Analisis Distribusi Fitur untuk Scaling:\n",
      "   • sepal length: Mean=5.84, Std=0.83, Skew=0.31\n",
      "   • sepal width: Mean=3.05, Std=0.43, Skew=0.33\n",
      "   • petal length: Mean=3.76, Std=1.76, Skew=-0.27\n",
      "   • petal width: Mean=1.20, Std=0.76, Skew=-0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perbandingan Hasil Scaling (Sepal Length):\n",
      "Method          Mean     Std      Min      Max      Range   \n",
      "-----------------------------------------------------------------\n",
      "Standard        -0.000  1.003   -1.870  2.492   4.362  \n",
      "Minmax          0.429   0.230   0.000   1.000   1.000  \n",
      "Robust          0.033   0.637   -1.154  1.615   2.769  \n",
      "None            5.843   0.828   4.300   7.900   3.600  \n",
      "\n",
      "FEATURE ENGINEERING:\n",
      "   Berdasarkan analisis korelasi dari DataUnderstanding:\n",
      "   • Petal length & petal width memiliki korelasi tinggi\n",
      "   • Sepal width memiliki korelasi rendah dengan fitur lain\n",
      "\n",
      "Feature Engineering Completed:\n",
      "   • Original features: 4\n",
      "   • Engineered features: 7\n",
      "   • Total features: 11\n",
      "\n",
      "Engineered Features:\n",
      "   1. petal_area\n",
      "   2. sepal_area\n",
      "   3. petal_sepal_length_ratio\n",
      "   4. petal_sepal_width_ratio\n",
      "   5. total_area\n",
      "   6. petal_aspect_ratio\n",
      "   7. sepal_aspect_ratio\n",
      "\n",
      "REKOMENDASI SCALING METHOD:\n",
      "   METODE YANG DIREKOMENDASIKAN: ROBUST\n",
      "   Alasan: RobustScaler direkomendasikan karena masih ada outliers\n",
      "   Deskripsi: RobustScaler (Median & IQR based)\n",
      "\n",
      "Dataset final dengan robust scaling siap untuk modeling\n",
      "Shape: (150, 19)\n",
      "Features: 11 total features\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling dan Engineering berdasarkan insights dari DataUnderstanding\n",
    "print(\"=== FEATURE SCALING DAN ENGINEERING ===\")\n",
    "\n",
    "if df_processed is not None:\n",
    "    \n",
    "    # Analisis distribusi fitur untuk menentukan scaling method\n",
    "    print(f\"\\nAnalisis Distribusi Fitur untuk Scaling:\")\n",
    "    for feature in features:\n",
    "        stats = df_processed[feature].describe()\n",
    "        skewness = df_processed[feature].skew()\n",
    "        print(f\"   • {feature.split('(')[0].strip()}: Mean={stats['mean']:.2f}, Std={stats['std']:.2f}, Skew={skewness:.2f}\")\n",
    "    \n",
    "    # Buat beberapa versi scaling\n",
    "    scaling_methods = {}\n",
    "    \n",
    "    # 1. StandardScaler (Z-score normalization)\n",
    "    scaler_standard = StandardScaler()\n",
    "    df_standard = df_processed.copy()\n",
    "    df_standard[features] = scaler_standard.fit_transform(df_processed[features])\n",
    "    scaling_methods['standard'] = {\n",
    "        'data': df_standard,\n",
    "        'scaler': scaler_standard,\n",
    "        'description': 'StandardScaler (Mean=0, Std=1)'\n",
    "    }\n",
    "    \n",
    "    # 2. MinMaxScaler (0-1 normalization)\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    df_minmax = df_processed.copy()\n",
    "    df_minmax[features] = scaler_minmax.fit_transform(df_processed[features])\n",
    "    scaling_methods['minmax'] = {\n",
    "        'data': df_minmax,\n",
    "        'scaler': scaler_minmax,\n",
    "        'description': 'MinMaxScaler (Range 0-1)'\n",
    "    }\n",
    "    \n",
    "    # 3. RobustScaler (robust to outliers)\n",
    "    scaler_robust = RobustScaler()\n",
    "    df_robust = df_processed.copy()\n",
    "    df_robust[features] = scaler_robust.fit_transform(df_processed[features])\n",
    "    scaling_methods['robust'] = {\n",
    "        'data': df_robust,\n",
    "        'scaler': scaler_robust,\n",
    "        'description': 'RobustScaler (Median & IQR based)'\n",
    "    }\n",
    "    \n",
    "    # 4. No scaling (original)\n",
    "    scaling_methods['none'] = {\n",
    "        'data': df_processed.copy(),\n",
    "        'scaler': None,\n",
    "        'description': 'No Scaling (Original values)'\n",
    "    }\n",
    "    \n",
    "    # Analisis hasil scaling\n",
    "    print(f\"\\nPerbandingan Hasil Scaling (Sepal Length):\")\n",
    "    feature_sample = 'sepal length (cm)'\n",
    "    print(f\"{'Method':<15} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Range':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for method_name, method_info in scaling_methods.items():\n",
    "        if feature_sample in method_info['data'].columns:\n",
    "            stats = method_info['data'][feature_sample].describe()\n",
    "            range_val = stats['max'] - stats['min']\n",
    "            print(f\"{method_name.title():<15} {stats['mean']:<7.3f} {stats['std']:<7.3f} {stats['min']:<7.3f} {stats['max']:<7.3f} {range_val:<7.3f}\")\n",
    "    \n",
    "    # Feature Engineering berdasarkan insights dari DataUnderstanding\n",
    "    print(f\"\\nFEATURE ENGINEERING:\")\n",
    "    print(f\"   Berdasarkan analisis korelasi dari DataUnderstanding:\")\n",
    "    print(f\"   • Petal length & petal width memiliki korelasi tinggi\")\n",
    "    print(f\"   • Sepal width memiliki korelasi rendah dengan fitur lain\")\n",
    "    \n",
    "    # Tambahkan engineered features untuk semua scaling methods\n",
    "    for method_name, method_info in scaling_methods.items():\n",
    "        df_current = method_info['data']\n",
    "        \n",
    "        # Feature engineering\n",
    "        # 1. Petal area (length × width)\n",
    "        df_current['petal_area'] = df_current['petal length (cm)'] * df_current['petal width (cm)']\n",
    "        \n",
    "        # 2. Sepal area (length × width)  \n",
    "        df_current['sepal_area'] = df_current['sepal length (cm)'] * df_current['sepal width (cm)']\n",
    "        \n",
    "        # 3. Petal to sepal ratio\n",
    "        df_current['petal_sepal_length_ratio'] = df_current['petal length (cm)'] / df_current['sepal length (cm)']\n",
    "        df_current['petal_sepal_width_ratio'] = df_current['petal width (cm)'] / df_current['sepal width (cm)']\n",
    "        \n",
    "        # 4. Total area\n",
    "        df_current['total_area'] = df_current['petal_area'] + df_current['sepal_area']\n",
    "        \n",
    "        # 5. Aspect ratios\n",
    "        df_current['petal_aspect_ratio'] = df_current['petal length (cm)'] / (df_current['petal width (cm)'] + 1e-8)\n",
    "        df_current['sepal_aspect_ratio'] = df_current['sepal length (cm)'] / (df_current['sepal width (cm)'] + 1e-8)\n",
    "        \n",
    "        # Update method info\n",
    "        scaling_methods[method_name]['data'] = df_current\n",
    "    \n",
    "    # Update features list\n",
    "    engineered_features = [\n",
    "        'petal_area', 'sepal_area', 'petal_sepal_length_ratio', \n",
    "        'petal_sepal_width_ratio', 'total_area', 'petal_aspect_ratio', 'sepal_aspect_ratio'\n",
    "    ]\n",
    "    \n",
    "    all_features = features + engineered_features\n",
    "    \n",
    "    print(f\"\\nFeature Engineering Completed:\")\n",
    "    print(f\"   • Original features: {len(features)}\")\n",
    "    print(f\"   • Engineered features: {len(engineered_features)}\")\n",
    "    print(f\"   • Total features: {len(all_features)}\")\n",
    "    print(f\"\\nEngineered Features:\")\n",
    "    for i, feat in enumerate(engineered_features, 1):\n",
    "        print(f\"   {i}. {feat}\")\n",
    "    \n",
    "    # Rekomendasi scaling method\n",
    "    print(f\"\\nREKOMENDASI SCALING METHOD:\")\n",
    "    \n",
    "    # Untuk Iris dataset dengan outliers yang sudah ditangani\n",
    "    if 'strong_consensus' in df_processed.columns and df_processed['strong_consensus'].sum() > 0:\n",
    "        recommended_scaling = 'robust'\n",
    "        reason = \"RobustScaler direkomendasikan karena masih ada outliers\"\n",
    "    else:\n",
    "        recommended_scaling = 'standard'\n",
    "        reason = \"StandardScaler optimal untuk data yang sudah bersih\"\n",
    "    \n",
    "    print(f\"   METODE YANG DIREKOMENDASIKAN: {recommended_scaling.upper()}\")\n",
    "    print(f\"   Alasan: {reason}\")\n",
    "    print(f\"   Deskripsi: {scaling_methods[recommended_scaling]['description']}\")\n",
    "    \n",
    "    # Pilih dataset final\n",
    "    df_final = scaling_methods[recommended_scaling]['data'].copy()\n",
    "    final_scaler = scaling_methods[recommended_scaling]['scaler']\n",
    "    \n",
    "    print(f\"\\nDataset final dengan {recommended_scaling} scaling siap untuk modeling\")\n",
    "    print(f\"Shape: {df_final.shape}\")\n",
    "    print(f\"Features: {len(all_features)} total features\")\n",
    "    \n",
    "else:\n",
    "    print(\"Tidak dapat melakukan feature scaling karena data processed tidak tersedia\")\n",
    "    df_final = None\n",
    "    final_scaler = None\n",
    "    all_features = features if 'features' in locals() else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbffce",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split dan Persiapan Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88b8e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN-TEST SPLIT DAN PERSIAPAN FINAL ===\n",
      "Dataset Preparation:\n",
      "   • Total samples: 150\n",
      "   • Features: 11\n",
      "   • Target classes: 3 (setosa=0, versicolor=1, virginica=2)\n",
      "\n",
      "Class Distribution:\n",
      "   • setosa: 50 samples (33.3%)\n",
      "   • versicolor: 50 samples (33.3%)\n",
      "   • virginica: 50 samples (33.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train-Test Split (80-20 dengan stratified sampling):\n",
      "   • Train set: 120 samples\n",
      "   • Test set: 30 samples\n",
      "\n",
      "Distribusi Kelas setelah Split:\n",
      "Class        Train Count  Train %    Test Count  Test %  \n",
      "-----------------------------------------------------------------\n",
      "setosa       40           33.3     % 10          33.3   %\n",
      "versicolor   40           33.3     % 10          33.3   %\n",
      "virginica    40           33.3     % 10          33.3   %\n",
      "\n",
      "Preprocessing Information Tersimpan:\n",
      "   • Outlier treatment: cap_outliers\n",
      "   • Scaling method: robust\n",
      "   • Total features: 11\n",
      "   • Random state: 42\n",
      "\n",
      "Sample Data Final (Train Set - First 5 rows):\n",
      "     sepal length (cm)  petal length (cm)  petal_area  total_area\n",
      "8            -1.076923          -0.842857    0.618095    0.833480\n",
      "106          -0.692308           0.042857    0.011429    0.703736\n",
      "76            0.769231           0.128571    0.008571   -0.299121\n",
      "9            -0.692308          -0.814286    0.651429    0.512967\n",
      "89           -0.230769          -0.100000   -0.000000    0.230769\n",
      "\n",
      "Target Labels (Train Set - First 10):\n",
      "[0, 2, 1, 0, 1, 2, 1, 2, 2, 2]\n",
      "\n",
      "PERSIAPAN UNTUK PYCARET MODELING:\n",
      "   df_pycaret: (120, 12) (untuk training & validation)\n",
      "   df_test_final: (30, 12) (untuk final evaluation)\n",
      "   Target column: 'species'\n",
      "   Feature columns: 11 features\n",
      "\n",
      "SUMMARY PREPROCESSING:\n",
      "   Objective: Multi-class classification (3 classes)\n",
      "   Data quality: High (outliers handled, scaled, engineered)\n",
      "   Features: Original (4) + Engineered (7) = 11\n",
      "   Class balance: Good (stratified split maintained)\n",
      "   Ready for: PyCaret automated ML pipeline\n",
      "\n",
      "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "Dataset siap untuk modeling dengan PyCaret\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split dan Persiapan Dataset untuk Modeling\n",
    "print(\"=== TRAIN-TEST SPLIT DAN PERSIAPAN FINAL ===\")\n",
    "\n",
    "if df_final is not None:\n",
    "    \n",
    "    # Persiapan features dan target\n",
    "    X = df_final[all_features].copy()\n",
    "    y = df_final['species'].copy()\n",
    "    \n",
    "    print(f\"Dataset Preparation:\")\n",
    "    print(f\"   • Total samples: {len(df_final)}\")\n",
    "    print(f\"   • Features: {len(all_features)}\")\n",
    "    print(f\"   • Target classes: {y.nunique()} (setosa=0, versicolor=1, virginica=2)\")\n",
    "    \n",
    "    # Cek distribusi kelas\n",
    "    class_distribution = y.value_counts().sort_index()\n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    species_names = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "    for class_idx, count in class_distribution.items():\n",
    "        pct = (count / len(y)) * 100\n",
    "        print(f\"   • {species_names[class_idx]}: {count} samples ({pct:.1f}%)\")\n",
    "    \n",
    "    # Train-test split dengan stratified sampling\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain-Test Split (80-20 dengan stratified sampling):\")\n",
    "    print(f\"   • Train set: {len(X_train)} samples\")\n",
    "    print(f\"   • Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # Verifikasi distribusi kelas di train dan test\n",
    "    train_dist = y_train.value_counts().sort_index()\n",
    "    test_dist = y_test.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nDistribusi Kelas setelah Split:\")\n",
    "    print(f\"{'Class':<12} {'Train Count':<12} {'Train %':<10} {'Test Count':<11} {'Test %':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for class_idx in [0, 1, 2]:\n",
    "        train_count = train_dist.get(class_idx, 0)\n",
    "        test_count = test_dist.get(class_idx, 0)\n",
    "        train_pct = (train_count / len(y_train)) * 100\n",
    "        test_pct = (test_count / len(y_test)) * 100\n",
    "        \n",
    "        print(f\"{species_names[class_idx]:<12} {train_count:<12} {train_pct:<9.1f}% {test_count:<11} {test_pct:<7.1f}%\")\n",
    "    \n",
    "    # Simpan informasi preprocessing untuk reproduksi\n",
    "    preprocessing_info = {\n",
    "        'outlier_treatment': 'cap_outliers',  # atau strategy yang dipilih\n",
    "        'scaling_method': 'robust',  # atau method yang dipilih\n",
    "        'original_features': features,\n",
    "        'engineered_features': engineered_features,\n",
    "        'all_features': all_features,\n",
    "        'scaler': final_scaler,\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPreprocessing Information Tersimpan:\")\n",
    "    print(f\"   • Outlier treatment: {preprocessing_info['outlier_treatment']}\")\n",
    "    print(f\"   • Scaling method: {preprocessing_info['scaling_method']}\")\n",
    "    print(f\"   • Total features: {len(preprocessing_info['all_features'])}\")\n",
    "    print(f\"   • Random state: {preprocessing_info['random_state']}\")\n",
    "    \n",
    "    # Tampilkan sample dari dataset final\n",
    "    print(f\"\\nSample Data Final (Train Set - First 5 rows):\")\n",
    "    sample_features = ['sepal length (cm)', 'petal length (cm)', 'petal_area', 'total_area']\n",
    "    print(X_train[sample_features].head())\n",
    "    \n",
    "    print(f\"\\nTarget Labels (Train Set - First 10):\")\n",
    "    print(y_train.head(10).tolist())\n",
    "    \n",
    "    # PyCaret Data Preparation\n",
    "    print(f\"\\nPERSIAPAN UNTUK PYCARET MODELING:\")\n",
    "    \n",
    "    # Gabungkan X dan y untuk PyCaret\n",
    "    df_pycaret = X_train.copy()\n",
    "    df_pycaret['species'] = y_train\n",
    "    \n",
    "    # Siapkan test set terpisah untuk evaluasi final\n",
    "    df_test_final = X_test.copy()\n",
    "    df_test_final['species'] = y_test\n",
    "    \n",
    "    print(f\"   df_pycaret: {df_pycaret.shape} (untuk training & validation)\")\n",
    "    print(f\"   df_test_final: {df_test_final.shape} (untuk final evaluation)\")\n",
    "    print(f\"   Target column: 'species'\")\n",
    "    print(f\"   Feature columns: {len(all_features)} features\")\n",
    "    \n",
    "    # Summary informasi untuk modeling\n",
    "    print(f\"\\nSUMMARY PREPROCESSING:\")\n",
    "    print(f\"   Objective: Multi-class classification (3 classes)\")\n",
    "    print(f\"   Data quality: High (outliers handled, scaled, engineered)\")\n",
    "    print(f\"   Features: Original (4) + Engineered (7) = {len(all_features)}\")\n",
    "    print(f\"   Class balance: Good (stratified split maintained)\")\n",
    "    print(f\"   Ready for: PyCaret automated ML pipeline\")\n",
    "    \n",
    "    print(f\"\\nPREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Dataset siap untuk modeling dengan PyCaret\")\n",
    "    \n",
    "else:\n",
    "    print(\"Tidak dapat melakukan train-test split karena dataset final tidak tersedia\")\n",
    "    X_train = X_test = y_train = y_test = None\n",
    "    df_pycaret = df_test_final = None\n",
    "    preprocessing_info = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fde19e",
   "metadata": {},
   "source": [
    "## 6. Summary dan Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a8a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                      SUMMARY DATA PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "DATA UNDERSTANDING INTEGRATION:\n",
      "   Berhasil mengintegrasikan hasil outlier detection dari DataUnderstanding\n",
      "   Multi-model outlier detection (ABOD, KNN, COF) dianalisis dan diterapkan\n",
      "   Consensus analysis digunakan untuk menentukan treatment strategy\n",
      "\n",
      "PREPROCESSING PIPELINE YANG DILAKUKAN:\n",
      "   1. Data Loading & Quality Check\n",
      "      • Dataset Iris: 150 samples, 4 original features\n",
      "      • No missing values, no duplicates\n",
      "      • Balanced classes (50 samples each)\n",
      "\n",
      "   2. Outlier Analysis & Treatment\n",
      "      • ABOD detected: 8 outliers (5.3%)\n",
      "      • KNN detected: 8 outliers (5.3%)\n",
      "      • COF detected: 8 outliers (5.3%)\n",
      "      • Consensus approach untuk robust treatment\n",
      "\n",
      "   3. Feature Scaling & Engineering\n",
      "      • Scaling: RobustScaler (optimal untuk outliers)\n",
      "      • Original features: 4\n",
      "      • Engineered features: 7 (areas, ratios, aspects)\n",
      "      • Total features: 11\n",
      "\n",
      "   4. Train-Test Split\n",
      "      • Stratified split: 80% train, 20% test\n",
      "      • Class distribution maintained\n",
      "      • Random state: 42 (reproducible)\n",
      "\n",
      "FINAL DATASET CHARACTERISTICS:\n",
      "   • Train samples: 120\n",
      "   • Test samples: 30\n",
      "   • Features: 11\n",
      "   • Classes: 3 (setosa, versicolor, virginica)\n",
      "   • Data quality: High (processed & validated)\n",
      "\n",
      "TECHNICAL SPECIFICATIONS:\n",
      "   • Outlier treatment: Based on multi-model consensus\n",
      "   • Scaling method: RobustScaler (median-based, outlier-resistant)\n",
      "   • Feature engineering: Domain-specific (botanical measurements)\n",
      "   • Cross-validation ready: Stratified sampling applied\n",
      "\n",
      "NEXT STEPS - MODELING PHASE:\n",
      "   1. Exploratory Data Analysis pada processed data\n",
      "   2. PyCaret Setup & Model Comparison\n",
      "      • Setup classification environment\n",
      "      • Compare multiple algorithms automatically\n",
      "      • Hyperparameter tuning & optimization\n",
      "\n",
      "   3. Model Training & Evaluation\n",
      "      • Train best performing models\n",
      "      • Cross-validation (k-fold)\n",
      "      • Feature importance analysis\n",
      "\n",
      "   4. Model Validation & Testing\n",
      "      • Final evaluation on test set\n",
      "      • Performance metrics (accuracy, precision, recall, F1)\n",
      "      • Confusion matrix analysis\n",
      "\n",
      "   5. Model Deployment Preparation\n",
      "      • Model finalization & saving\n",
      "      • Preprocessing pipeline serialization\n",
      "      • Documentation & deployment notes\n",
      "\n",
      "REKOMENDASI UNTUK MODELING:\n",
      "   • Gunakan PyCaret untuk automated ML workflow\n",
      "   • Focus pada ensemble methods (Random Forest, XGBoost)\n",
      "   • Monitor overfitting dengan validation curves\n",
      "   • Analyze feature importance untuk interpretability\n",
      "\n",
      "AVAILABLE DATASETS FOR MODELING:\n",
      "   df_pycaret: Training data untuk PyCaret setup\n",
      "   df_test_final: Hold-out test set untuk final evaluation\n",
      "   preprocessing_info: Pipeline metadata untuk reproduksi\n",
      "\n",
      "DOCUMENTATION & REPRODUCIBILITY:\n",
      "   • All preprocessing steps documented dengan kode\n",
      "   • Random states fixed untuk reproducibility\n",
      "   • Scaler objects tersimpan untuk inference\n",
      "   • Feature engineering pipeline dapat direplikasi\n",
      "\n",
      "================================================================================\n",
      "                      PREPROCESSING COMPLETED\n",
      "                    READY FOR MODELING PHASE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# SUMMARY PREPROCESSING DAN NEXT STEPS\n",
    "print(\"=\" * 80)\n",
    "print(\"                      SUMMARY DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nDATA UNDERSTANDING INTEGRATION:\")\n",
    "print(\"   Berhasil mengintegrasikan hasil outlier detection dari DataUnderstanding\")\n",
    "print(\"   Multi-model outlier detection (ABOD, KNN, COF) dianalisis dan diterapkan\")\n",
    "print(\"   Consensus analysis digunakan untuk menentukan treatment strategy\")\n",
    "\n",
    "print(\"\\nPREPROCESSING PIPELINE YANG DILAKUKAN:\")\n",
    "print(\"   1. Data Loading & Quality Check\")\n",
    "print(\"      • Dataset Iris: 150 samples, 4 original features\")\n",
    "print(\"      • No missing values, no duplicates\")\n",
    "print(\"      • Balanced classes (50 samples each)\")\n",
    "\n",
    "print(\"\\n   2. Outlier Analysis & Treatment\")\n",
    "print(\"      • ABOD detected: 8 outliers (5.3%)\")  \n",
    "print(\"      • KNN detected: 8 outliers (5.3%)\")\n",
    "print(\"      • COF detected: 8 outliers (5.3%)\")\n",
    "print(\"      • Consensus approach untuk robust treatment\")\n",
    "\n",
    "print(\"\\n   3. Feature Scaling & Engineering\")\n",
    "print(\"      • Scaling: RobustScaler (optimal untuk outliers)\")\n",
    "print(\"      • Original features: 4\")\n",
    "print(\"      • Engineered features: 7 (areas, ratios, aspects)\")\n",
    "print(\"      • Total features: 11\")\n",
    "\n",
    "print(\"\\n   4. Train-Test Split\")\n",
    "print(\"      • Stratified split: 80% train, 20% test\")\n",
    "print(\"      • Class distribution maintained\")\n",
    "print(\"      • Random state: 42 (reproducible)\")\n",
    "\n",
    "if 'preprocessing_info' in locals() and preprocessing_info is not None:\n",
    "    print(f\"\\nFINAL DATASET CHARACTERISTICS:\")\n",
    "    print(f\"   • Train samples: {preprocessing_info.get('train_size', 'N/A')}\")\n",
    "    print(f\"   • Test samples: {preprocessing_info.get('test_size', 'N/A')}\")\n",
    "    print(f\"   • Features: {len(preprocessing_info.get('all_features', []))}\")\n",
    "    print(f\"   • Classes: 3 (setosa, versicolor, virginica)\")\n",
    "    print(f\"   • Data quality: High (processed & validated)\")\n",
    "\n",
    "print(f\"\\nTECHNICAL SPECIFICATIONS:\")\n",
    "print(f\"   • Outlier treatment: Based on multi-model consensus\")\n",
    "print(f\"   • Scaling method: RobustScaler (median-based, outlier-resistant)\")\n",
    "print(f\"   • Feature engineering: Domain-specific (botanical measurements)\")\n",
    "print(f\"   • Cross-validation ready: Stratified sampling applied\")\n",
    "\n",
    "print(f\"\\nNEXT STEPS - MODELING PHASE:\")\n",
    "print(f\"   1. Exploratory Data Analysis pada processed data\")\n",
    "print(f\"   2. PyCaret Setup & Model Comparison\")\n",
    "print(f\"      • Setup classification environment\")\n",
    "print(f\"      • Compare multiple algorithms automatically\")\n",
    "print(f\"      • Hyperparameter tuning & optimization\")\n",
    "\n",
    "print(f\"\\n   3. Model Training & Evaluation\")\n",
    "print(f\"      • Train best performing models\")\n",
    "print(f\"      • Cross-validation (k-fold)\")\n",
    "print(f\"      • Feature importance analysis\")\n",
    "\n",
    "print(f\"\\n   4. Model Validation & Testing\")\n",
    "print(f\"      • Final evaluation on test set\")\n",
    "print(f\"      • Performance metrics (accuracy, precision, recall, F1)\")\n",
    "print(f\"      • Confusion matrix analysis\")\n",
    "\n",
    "print(f\"\\n   5. Model Deployment Preparation\")\n",
    "print(f\"      • Model finalization & saving\")\n",
    "print(f\"      • Preprocessing pipeline serialization\")\n",
    "print(f\"      • Documentation & deployment notes\")\n",
    "\n",
    "print(f\"\\nREKOMENDASI UNTUK MODELING:\")\n",
    "print(f\"   • Gunakan PyCaret untuk automated ML workflow\")\n",
    "print(f\"   • Focus pada ensemble methods (Random Forest, XGBoost)\")\n",
    "print(f\"   • Monitor overfitting dengan validation curves\")\n",
    "print(f\"   • Analyze feature importance untuk interpretability\")\n",
    "\n",
    "print(f\"\\nAVAILABLE DATASETS FOR MODELING:\")\n",
    "if 'df_pycaret' in locals() and df_pycaret is not None:\n",
    "    print(f\"   df_pycaret: Training data untuk PyCaret setup\")\n",
    "    print(f\"   df_test_final: Hold-out test set untuk final evaluation\")\n",
    "    print(f\"   preprocessing_info: Pipeline metadata untuk reproduksi\")\n",
    "else:\n",
    "    print(f\"   Datasets belum tersedia - run preprocessing cells terlebih dahulu\")\n",
    "\n",
    "print(f\"\\nDOCUMENTATION & REPRODUCIBILITY:\")\n",
    "print(f\"   • All preprocessing steps documented dengan kode\")\n",
    "print(f\"   • Random states fixed untuk reproducibility\")\n",
    "print(f\"   • Scaler objects tersimpan untuk inference\")\n",
    "print(f\"   • Feature engineering pipeline dapat direplikasi\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"                      PREPROCESSING COMPLETED\")\n",
    "print(f\"                    READY FOR MODELING PHASE\")\n",
    "print(f\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
